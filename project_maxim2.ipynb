{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "272a5166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31f7b9fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>1753918954</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@JohnLloydTaylor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>1753919001</td>\n",
       "      <td>love</td>\n",
       "      <td>Happy Mothers Day  All my love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>1753919005</td>\n",
       "      <td>love</td>\n",
       "      <td>Happy Mother's Day to all the mommies out ther...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>1753919043</td>\n",
       "      <td>happiness</td>\n",
       "      <td>@niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>1753919049</td>\n",
       "      <td>love</td>\n",
       "      <td>@mopedronin bullet train from tokyo    the gf ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id   sentiment  \\\n",
       "0      1956967341       empty   \n",
       "1      1956967666     sadness   \n",
       "2      1956967696     sadness   \n",
       "3      1956967789  enthusiasm   \n",
       "4      1956968416     neutral   \n",
       "...           ...         ...   \n",
       "39995  1753918954     neutral   \n",
       "39996  1753919001        love   \n",
       "39997  1753919005        love   \n",
       "39998  1753919043   happiness   \n",
       "39999  1753919049        love   \n",
       "\n",
       "                                                 content  \n",
       "0      @tiffanylue i know  i was listenin to bad habi...  \n",
       "1      Layin n bed with a headache  ughhhh...waitin o...  \n",
       "2                    Funeral ceremony...gloomy friday...  \n",
       "3                   wants to hang out with friends SOON!  \n",
       "4      @dannycastillo We want to trade with someone w...  \n",
       "...                                                  ...  \n",
       "39995                                   @JohnLloydTaylor  \n",
       "39996                     Happy Mothers Day  All my love  \n",
       "39997  Happy Mother's Day to all the mommies out ther...  \n",
       "39998  @niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...  \n",
       "39999  @mopedronin bullet train from tokyo    the gf ...  \n",
       "\n",
       "[40000 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('tweet_emotions.csv', encoding='ISO_8859_1')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d687e2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e48d484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a516aebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dc1c729",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextNormalizer(BaseEstimator,TransformerMixin):\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        X_copy = X.copy()\n",
    "        for i in range(len(X)):\n",
    "            X_copy[i] = ' '.join([token.lower()\n",
    "                                 for token in word_tokenize(X_copy[i])])\n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f37eec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = TextNormalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34df2ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hellow , my name , children']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm.fit_transform ([\"Hellow, my name, children\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "247c3344",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b90f49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, stop_words):\n",
    "        self.stop_words = stop_words\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        self.general_freq = FreqDist()\n",
    "        for document in X:\n",
    "            tokens = word_tokenize(document,language = \"russian\")\n",
    "            freq = FreqDist(tokens)\n",
    "            self.general_freq.update(freq)\n",
    "        self.hapaxes = self.general_freq.hapaxes()\n",
    "        return self\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        X_copy = X.copy()\n",
    "        for i in range(len(X)):\n",
    "            X_copy[i] = ' '.join([token for token in word_tokenize(X[i])\n",
    "                                 if token not in self.hapaxes and\n",
    "                                 token not in self.stop_words])\n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d50e9877",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cf2a460",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_extractor = WordExtractor(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed653c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'Children drink milk',\n",
    "     'I love milk',\n",
    "      'My children play together',\n",
    "    'It-s fun to play together'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c94136e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['milk', 'milk', 'play together', 'play together']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_extractor.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1c92218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApplyStemmer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, stemmer):\n",
    "        self.stemmer = stemmer\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_tranform):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None, **fit_tranform):\n",
    "        X_copy = X.copy()\n",
    "        for i in range(len(X)):\n",
    "            X_copy[i] = ' '.join([self.stemmer.stem(token) \n",
    "                                  for token in word_tokenize(X_copy[i])])\n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27863cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3404e1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball_stemmer = SnowballStemmer(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee54f4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_stemmer=ApplyStemmer(snowball_stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60e4360d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Drink milk , children , you will be healthy !']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_stemmer.fit_transform(['Drink milk, children, you will be healthy!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30f040cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c46cd7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "338ce264",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "   (\"norm\", TextNormalizer()),\n",
    "    (\"extractor\", WordExtractor(stop_words)),\n",
    "    (\"stemmer\", ApplyStemmer(PorterStemmer())),\n",
    "    (\"vectorizer\", TfidfVectorizer()),\n",
    "    (\"logic\", LogisticRegression(max_iter=1000))\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f092e31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['content'].values\n",
    "y = df['sentiment'].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cac5fe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40360753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;norm&#x27;, TextNormalizer()),\n",
       "                (&#x27;extractor&#x27;,\n",
       "                 WordExtractor(stop_words=[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;,\n",
       "                                           &#x27;our&#x27;, &#x27;ours&#x27;, &#x27;ourselves&#x27;, &#x27;you&#x27;,\n",
       "                                           &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;, &quot;you&#x27;ll&quot;,\n",
       "                                           &quot;you&#x27;d&quot;, &#x27;your&#x27;, &#x27;yours&#x27;, &#x27;yourself&#x27;,\n",
       "                                           &#x27;yourselves&#x27;, &#x27;he&#x27;, &#x27;him&#x27;, &#x27;his&#x27;,\n",
       "                                           &#x27;himself&#x27;, &#x27;she&#x27;, &quot;she&#x27;s&quot;, &#x27;her&#x27;,\n",
       "                                           &#x27;hers&#x27;, &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;,\n",
       "                                           &#x27;its&#x27;, &#x27;itself&#x27;, ...])),\n",
       "                (&#x27;stemmer&#x27;, ApplyStemmer(stemmer=&lt;PorterStemmer&gt;)),\n",
       "                (&#x27;vectorizer&#x27;, TfidfVectorizer()),\n",
       "                (&#x27;logic&#x27;, LogisticRegression(max_iter=1000))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;norm&#x27;, TextNormalizer()),\n",
       "                (&#x27;extractor&#x27;,\n",
       "                 WordExtractor(stop_words=[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;,\n",
       "                                           &#x27;our&#x27;, &#x27;ours&#x27;, &#x27;ourselves&#x27;, &#x27;you&#x27;,\n",
       "                                           &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;, &quot;you&#x27;ll&quot;,\n",
       "                                           &quot;you&#x27;d&quot;, &#x27;your&#x27;, &#x27;yours&#x27;, &#x27;yourself&#x27;,\n",
       "                                           &#x27;yourselves&#x27;, &#x27;he&#x27;, &#x27;him&#x27;, &#x27;his&#x27;,\n",
       "                                           &#x27;himself&#x27;, &#x27;she&#x27;, &quot;she&#x27;s&quot;, &#x27;her&#x27;,\n",
       "                                           &#x27;hers&#x27;, &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;,\n",
       "                                           &#x27;its&#x27;, &#x27;itself&#x27;, ...])),\n",
       "                (&#x27;stemmer&#x27;, ApplyStemmer(stemmer=&lt;PorterStemmer&gt;)),\n",
       "                (&#x27;vectorizer&#x27;, TfidfVectorizer()),\n",
       "                (&#x27;logic&#x27;, LogisticRegression(max_iter=1000))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TextNormalizer</label><div class=\"sk-toggleable__content\"><pre>TextNormalizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">WordExtractor</label><div class=\"sk-toggleable__content\"><pre>WordExtractor(stop_words=[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;, &#x27;our&#x27;, &#x27;ours&#x27;,\n",
       "                          &#x27;ourselves&#x27;, &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;, &quot;you&#x27;ll&quot;,\n",
       "                          &quot;you&#x27;d&quot;, &#x27;your&#x27;, &#x27;yours&#x27;, &#x27;yourself&#x27;, &#x27;yourselves&#x27;,\n",
       "                          &#x27;he&#x27;, &#x27;him&#x27;, &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;, &quot;she&#x27;s&quot;, &#x27;her&#x27;,\n",
       "                          &#x27;hers&#x27;, &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;, &#x27;itself&#x27;, ...])</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ApplyStemmer</label><div class=\"sk-toggleable__content\"><pre>ApplyStemmer(stemmer=&lt;PorterStemmer&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('norm', TextNormalizer()),\n",
       "                ('extractor',\n",
       "                 WordExtractor(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
       "                                           'our', 'ours', 'ourselves', 'you',\n",
       "                                           \"you're\", \"you've\", \"you'll\",\n",
       "                                           \"you'd\", 'your', 'yours', 'yourself',\n",
       "                                           'yourselves', 'he', 'him', 'his',\n",
       "                                           'himself', 'she', \"she's\", 'her',\n",
       "                                           'hers', 'herself', 'it', \"it's\",\n",
       "                                           'its', 'itself', ...])),\n",
       "                ('stemmer', ApplyStemmer(stemmer=<PorterStemmer>)),\n",
       "                ('vectorizer', TfidfVectorizer()),\n",
       "                ('logic', LogisticRegression(max_iter=1000))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e32529ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "wypred=pipe.predict (X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de0f65ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3423"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score (wypred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c720842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c868a9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump (pipe,open (\"pipe.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e87119",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
